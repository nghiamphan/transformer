# Simple Transformer Implementation

## Table of Contents

1. [Problem](#problem)
2. [How to run](#how-to-run)
3. [Encoder-only Transformer](#encoder-only-transformer)
    1. [Data Generation](#data-generation)
    2. [Transformer Architecture](#encoder-only-transformer-architecture)
    3. [Model Training](#model-training)
    4. [Model Evaluation](#model-evaluation)
    5. [Model Tuning](#model-tuning)
4. [Vanilla Transformer](#vanilla-transformer)
    1. [Data Generation](#vanilla-data-generation)
    2. [Transformer Architecture](#vanilla-transformer-architecture)
    3. [Model Training](#vanilla-model-training)
    4. [Model Evaluation](#vanilla-model-evaluation)
        1. [Initial Observation](#initial-observation)
    5. [Analysis & Improvement](#vanilla-improvement)
5. [Model Comparison](#model-comparison)

# Problem

Implement a transformer to reverse a sequence of integers.

For example: If the input sequence is [3, 5, 4, 2, 1], the transformer should return [1, 2, 4, 5, 3]

# How to run

Run the Jupyter notebook file <b>main.ipynb</b> for the encoder-only transformer.

Run <b>vanilla_main.ipynb</b> for a version of transformer with both encoder and decoder (aka vanilla transformer).

# Encoder-only Transformer

## Data Generation

First, we randomly generate data using class CustomDataSet. It takes three important parameters:

-   sample_size_by_seq_length: dict[seq_length: sample_size]

    This is a dictionary where a key is a sequence length and its value is the number of sequences to be generated by that sequence length. <br>
    Example: {1: 4, 2: 20, 3: 100, 4: 400, 5: 1000}

-   max_seq_length: int

    the maximum length of each sequence

-   vocab_size: int

    the size of vocabulary to be generated

Each input sequence will be randomly generated given a specific length and consists of integers from 1 to <i>vocab_size</i>. Its corresponding target will be its reverse. <br>
If an input sequence and its target have length < <i>max_seq_length</i>, it will be padded with 0s at the end so that all inputs and outputs will have length = <i>max_seq_length</i>. <br>
Duplicated input generations will also be dropped.

For the purpose of training and testing our models, we choose <i>max_seq_length = 5</i> and <i>vocab_size = 9</i> (actual vocabulary contains tokens from 1 to 9).

Example:

-   input = [1, 2, 3, 4, 5], target = [5, 4, 3, 2, 1]
-   input = [1, 2, 3, 0, 0], target = [3, 2, 1, 0, 0]

## Encoder-only Transformer Architecture

![alt text](<photos/Encoder-only Transformer.png>)

The transformer has the following parameters:

-   src_vocab_size: int

    Vocabulary size of the input

-   tgt_vocab_size: int

    Vocabulary size of the output

-   embed_dim: int

    A dimension of many layers of the transformer

-   max_seq_length: int

    Maximum sequence length of an input

-   n_heads: int

    Number of attention heads

-   n_layers: int

    Number of encoder layers

-   d_ff: int

    Dimension of the inner layer of the feed forward component

-   dropout_rate: float

    Dropout rate applied on output of self attention and feed forward component

-   apply_mask: bool

    Whether to apply mask on the input. If True, a token in the input sequence will only attend to non-padding tokens including itself. If False, it will attend to every token including padding tokens in the sequence.

### 1. Input

Input is sequence of integers. Example: [3, 5, 4, 2, 1].

### 2. Embedding Layer

If the input sequence has length < <i>max_seq_length</i>, pad 0s at the end. <br>
Embed each integer in the sequence into a vector of dimension <i>embed_dim</i>.

### 3. Positional Encoding

Encode each position from 0 to <i>max_seq_length-1</i> into a vector of dimension <i>embed_dim</i>.
The positional encoding is fixed and calculated by the following formula:

$$PE(pos, 2i) = sin(\frac {pos} {10000^{\frac {2i} {embed\_dim}}})$$
$$PE(pos, 2i+1) = cos(\frac {pos} {10000^{\frac {2i} {embed\_dim}}})$$

where pos = 0,..., max_seq_length and i = 0,..., embed_dim // 2

The output after the positional encoding layer is the sum of the output of the embedding layer and the positional encoding.

### 4. Encoder Block

Each encoder block consists of four components: multihead attention layer, first normalization layer, feed forward layer, and second normalization layer. <br>
Encoder block can be duplicated as specified by parameter <i>n_layers</i>.

#### 4.1. Multihead Attention

Purpose: the tokens within a sequence can have multiple relationships with each other at the same time. By using multihead attention, we try to capture those different relationships.

The multihead attention takes in three parameters:

-   $X_q$: matrix to be projected into a query matrix by multiplying with the learned matrix $W^Q$
-   $X_q$: matrix to be projected into a key matrix by multiplying with the learned matrix $W^K$
-   $X_v$: matrix to be projected into a value matrix by multiplying with the learned matrix $W^V$

In the encoder block, all these three parameters are the same and equal to the output after the positional encoding layer.

<b>How multihead attention is calculated in theory</b> <br>
Let <i>X</i> be the output after the positional encoding layer. <br>
Each attention head has a unique set of learned matrices $W_i^Q$, $W_i^K$, $W_i^V$ which is used to project $Q_i$, $K_i$, and $V_i$ as follows:

$$Q_i = W_i^QX$$
$$K_i = W_i^KX$$
$$V_i = W_i^VX$$

Attention score is calculated for each head (mask will be applied if parameter <i>apply_mask=True</i>):
$$Attention Score_i = softmax(\frac {Q_i K_i^T} {\sqrt {embed\_dim}})V_i$$

Then the attention scores of all attention heads at are concatenated into one single matrix to be fed to the next layer.

#### 4.2. Normalization 1

The output of multihead attention and of positional encoding layer are added together and fed into a normalization layer. <br>
To prevent overfitting, dropout with <i>dropout_rate</i> is applied on the multihead attention layer's output.

#### 4.3. Feed Forward

The feed forward component consists of:

-   a linear layer with dimension <i>d_ff</i> and ReLU activation
-   a linear layer with dimension <i>embed_dim</i>.

#### 4.4. Normalization 2

The output of feed forward layer and of the first normalization layer are added together and fed into a second normalization layer. <br>
To prevent overfitting, dropout with <i>dropout_rate</i> is applied on the feed forward layer's output.

### 5. Linear Layer

A linear layer that transforms the dimension of the output from encoder block from <i>embed_dim</i> to <i>target_vocab_size</i>.

### 6. Softmax

Apply softmax function to get the probability of each token in the target vocabulary. <br>

### Summary

The transformer has:

-   input: a sequence of integers. Dimension: <i>[seq_length]</i>.
-   output: probability distribution over the target vocabulary for each token in the output sequence. Dimension: <i>[max_seq_length, target_vocab_size]</i>.

Note: in the actual implementation, we do not apply softmax function because when we use pytorch's CrossEntropyLoss() to calculate the loss and pytorch's CrossEntropyLoss() already applies softmax function before calculating the loss.

## Model Training

We train the model on train dataset and use cross entropy loss as objective function.

## Model Evaluation

We run the model on test dataset, and report the cross entropy loss.<br>
Besides, we also report prediction accuracy on token and sequence level.

## Model Tuning

We use optuna and a validation dataset to tune the model for the following parameters (numbers in brackets are possible values):

-   embed_dim: [256, 512, 1024, 2048]
-   n_heads: [1, 2, 4, 8]
-   n_layers: [1, 2, 4, 8]
-   d_ff = [512, 1024, 2048, 4096]
-   dropout_rate: [0, 0.1, 0.2, 0.3, 0.4, 0.5]
-   apply_mask : [True, False]

We observe that the most impactful parameters is the number of encoders (<i>n_layers</i>). When <i>n_layers = 1</i>, the accuracy on token level on the test dataset is about 66% after 20 training epochs. When <i>n_layers = 4</i>, the accuracy is over 99%.

# Vanilla Transformer

## Data Generation <a id="vanilla-data-generation"></a>

We use the same data generation that we use for the encoder-only model, except that we also add a starting token at the beginning of the target sequence. The starting token equals to the largest number in the actual vocabulary + 1.

Example: when <i>vocab_size = 9</i>, vocabulary contains tokens 1 to 9, starting token is 10.

-   input = [1, 2, 3, 4, 5], target = [10, 5, 4, 3, 2, 1]
-   input = [1, 2, 3, 0, 0], target = [10, 3, 2, 1, 0, 0]

## Vanilla Transformer Architecture

![alt text](<photos/Vanilla Transformer.png>)

## Model Training <a id="vanilla-model-training"></a>

Example:

-   encoder input = [1, 2, 3, 4, 5], decoder input = [10, 5, 4, 3, 2], model target = [5, 4, 3, 2, 1]
-   encoder input = [1, 2, 3, 0, 0], decoder input = [10, 3, 2, 1, 0], model target = [3, 2, 1, 0, 0]

## Model Evaluation <a id="vanilla-model-evaluation"></a>

In testing, output will be generated token by token. This is different from the encoder-only model, where the entire output sequence will be generated all at once.

Example: input = [1, 2, 3, 4, 5]

-   First iteration: encoder input = [1, 2, 3, 4, 5], decoder input = [10] -> output = [5]
-   Second iteration: encoder input = [1, 2, 3, 4, 5], decoder input = [10, 5] -> output = [5, 4]
-   ...
-   The model continues generating each new token until output's length = input's length.

### Initial Observation

1. In the full model, there is a cross attention block within the decoder which takes matrix Q from the decoder itself but takes matrix K and V from the encoder. When Q is smaller in dimension than K (e.g.: encoder input is [1, 2, 3, 4, 5] but decoder input is [10, 5]), the product of Q and K (which is part of the equation for attention score) will have the smaller dimension of Q (that how pytorchâ€™s implementation works, the extra portion in matrix K will simply be ignored).
   In other words, for example, when we generate the second token for the output, it will only pay attention to the first 2 tokens of the encoder input (e.g.: [1, 2] in this case). It makes sense for a translation problem, but you would think that for this problem, the model should pay attention to the entire sequence of input at any iteration.

2. Regardless of the previous point, the vanilla transformer performs way better when there are fewer layers.
   When there is only one layer of encoder and/or decoder, the encoder-only model has accuracy of 7% on sequence level, vs. 83% for the vanilla model (trained and tested on the same data).

3. Increasing the number of layers is the most important factor in improving performance in both models. When the number of layers = 4, the accuracy in both cases is close to 100%.

## Analysis & Improvement <a id="vanilla-improvement"></a>

1.  We want to improve on the first point of the initial observation. The seemingly obvious idea is to reverse matrix K and V that are passed from the encoder to the cross-attention block of the decoder. This simple modification immediately improves the accuracy on sequences with 5 tokens to 100%.

    However...

    In our data, for sequences with fewer than 5 tokens, we pad 0s at the end, and this modification does not work very well on those sequences.
    For example: when input = [1, 2, 3, 0, 0], to generate the first token, the model will only pay attention to the paddng 0 at the end, and this information would not be enough.
    In fact, blindly reverse matrix K and V will never let the model achieve nearly 100% accuracy no matter how many layers we have.

2.  So for the next idea, we reverse matrix K and V such that to generate the first token, the model will pay attention to the actual last token of the input and not the padding 0.
    For example: if input = [1, 2, 3, 0, 0]:

    -   First iteration: pays attention to [3]
    -   Second iteration: pays attention to [3, 2]
    -   Third iteration: pays attention to [3, 2, 1]
    -   Fourth iteration: pays attention to [3, 2, 1, 0 ]
    -   Fifth iteration: pays attention to [3, 2, 1, 0, 0]

    For reasons that still elude me, this version of reverse does NOT perform any better than the version without any reverse at all. It still, however, achieves nearly 100% accuracy with 4 layers.

3.  The next idea we have is to remove padding 0s from the input before passing it to the forward propagation of the model.
    For exampple, if input = [1, 2, 3, 0, 0]

    -   During training, encoder input = [1, 2, 3], decoder input = [10, 3, 2], model target = [3, 2, 1]
    -   During testing, encoder input = [1, 2, 3]. First iteration, decoder input = [10]. The model will run 3 iterations to generate the output.

    As we already remove the padding 0s from input, simply reversing matrix K and V that are passed from the encoder to the cross-attention block of the decoder is enough.

    One big drawback of this modification is that since the input lengths are now variable, we cannot do batch training and testing anymore. Thus it takes significantly longer to train and test the model.

    Another important note about this model is that it seems to achieve best performance with 2 layers. Adding more layers than 2 will actually decrease its performance.

# Model Comparison

We will compare the performance of the following models:

-   Encoder-only transformer
-   Vanilla transformer with no modification
-   Vanilla transformer with simple reverse of matrix K and V, as described in section 1 of [Analysis & Improvement](#vanilla-improvement)
-   Vanilla transformer with reverse and modification 1, as described in section 2 of [Analysis & Improvement](#vanilla-improvement)
-   Vanilla transformer with reverse and modification 2, as described in section 3 of [Analysis & Improvement](#vanilla-improvement)

All the training and testing data are the same. All parameters of those models are kept the same as follows:

-   actual vocab_size = 9 (tokens 1 to 9)
-   embed_dim = 512
-   max_seq_length = 5
-   n_heads = 8
-   d_ff = 2048
-   dropout_rate = 0.1
-   training epochs = 20

The key metric is accuracy on sequence level of the above models with different numbers of layers.

|            | Encoder-only |        | Vanilla             |               |               |
| ---------- | ------------ | ------ | ------------------- | ------------- | ------------- |
|            |              | No mod | Simple reverse      | Reverse mod 1 | Reverse mod 2 |
| Git branch | main         | main   | vanilla-transformer | experiment-1  | experiment-2  |
|            |              |        |                     |               |               |
| 1 layer    | 7%           | 83%    | 95%                 | 83%           | 89%           |
| 2 layers   | >99%         | >99%   | 97%                 | >99%          | >99.9%        |
| 4 layers   | >99.9%       | >99.9% | 97%                 | >99.9%        | 98%           |

Note: The only reason we explore model "reverse mod 1" and "reverse mod 2" is because the real inputs have variable lengths (not counting padding tokens). If all inputs have a fixed length, then the model "simple reverse" will achieve 100% accuracy with just one layer.
