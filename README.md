# Simple Transformer Implementation

<b>Problem</b>: Implement a transformer to reverse a sequence of integers.

For example: If the input sequence is [3, 5, 4, 2, 1], the transformer should return [1, 2, 4, 5, 3]

## How to run
Run the Jupyter notebook file <b>main.ipynb</b>.

## Data Generation

First, we randomly generate data using class CustomDataSet. It takes three important parameters:
- sample_size_by_seq_length: dict[seq_length: sample_size]

    This is a dictionary where a key is a sequence length and its value is the number of sequences to be generated by that sequence length. <br>
    Example: {1: 4, 2: 20, 3: 100, 4: 400, 5: 1000}

- max_seq_length: int

    the maximum length of each sequence

- vocab_size: int

    the size of vocabulary to be generated

Each input sequence will be randomly generated given a specific length and consists of integers from 1 to <i>vocab_size</i>. Its corresponding target will be its reverse. <br>
If an input sequence and its target have length < <i>max_seq_length</i>, it will be padded with 0s at the end so that all inputs and outputs will have length = <i>max_seq_length</i>. <br>
Duplicated input generations will also be dropped.

## Transformer Architecture

![Alt text](<Encoder-only Transformer.png>)

The transformer has the following parameters:
- src_vocab_size: int

    Vocabulary size of the input

- tgt_vocab_size: int

    Vocabulary size of the output

- embed_dim: int

    A dimension of many layers of the transformer

- max_seq_length: int

    Maximum sequence length of an input

- n_heads: int

    Number of attention heads

- n_layers: int

    Number of encoder layers

- d_ff: int

    Dimension of the inner layer of the feed forward component

- dropout_rate: float

    Dropout rate applied on output of self attention and feed forward component

- apply_mask: bool

    Whether to apply mask on the input. If True, a token in the input sequence will only attend to non-padding tokens including itself. If False, it will attend to every token including padding tokens in the sequence.

### 1. Input
Input is sequence of integers. Example: [3, 5, 4, 2, 1].

### 2. Embedding Layer
If the input sequence has length < <i>max_seq_length</i>, pad 0s at the end. <br>
Embed each integer in the sequence into a vector of dimension <i>embed_dim</i>.

### 3. Positional Encoding
Encode each position from 0 to <i>max_seq_length-1</i> into a vector of dimension <i>embed_dim</i>.
The positional encoding is fixed and calculated by the following formula:

$$PE(pos, 2i) = sin(\frac {pos} {10000^{\frac {2i} {embed\_dim}}})$$
$$PE(pos, 2i+1) = cos(\frac {pos} {10000^{\frac {2i} {embed\_dim}}})$$

where pos = 0,..., max_seq_length and i = 0,..., embed_dim // 2

The output after the positional encoding layer is the sum of the output of the embedding layer and the positional encoding.

### 4. Encoder Block

Each encoder block consists of four components: multihead attention layer, first normalization layer, feed forward layer, and second normalization layer. <br>
Encoder block can be duplicated as specified by parameter <i>n_layers</i>.

#### 4.1 Multihead Attention
Purpose: the tokens within a sequence can have multiple relationships with each other at the same time. By using multihead attention, we try to capture those different relationships.

The multihead attention takes in three parameters:
- $X_q$: matrix to be projected into a query matrix by multiplying with the learned matrix $W^Q$
- $X_q$: matrix to be projected into a key matrix by multiplying with the learned matrix $W^K$
- $X_v$: matrix to be projected into a value matrix by multiplying with the learned matrix $W^V$

In the encoder block, all these three parameters are the same and equal to the output after the positional encoding layer.

<b>How multihead attention is calculated in theory</b> <br>
Let <i>X</i> be the output after the positional encoding layer. <br>
Each attention head has a unique set of learned matrices $W_i^Q$, $W_i^K$, $W_i^V$ which is used to project $Q_i$, $K_i$, and $V_i$ as follows:

$$Q_i = W_i^QX$$
$$K_i = W_i^KX$$
$$V_i = W_i^VX$$

Attention score is calculated for each head (mask will be applied if parameter <i>apply_mask=True</i>):
$$Attention Score_i = softmax(\frac {Q_i K_i^T} {\sqrt {embed\_dim}})V_i$$

Then the attention scores of all attention heads at are concatenated into one single matrix to be fed to the next layer.

#### 4.2. Normalization 1
The output of multihead attention and of positional encoding layer are added together and fed into a normalization layer. <br>
To prevent overfitting, dropout with <i>dropout_rate</i> is applied on the multihead attention layer's output.

#### 4.3. Feed Forward
The feed forward component consists of:
- a linear layer with dimension <i>d_ff</i> and ReLU activation
- a linear layer with dimension <i>embed_dim</i>.

#### 4.4. Normalization 2
The output of feed forward layer and of the first normalization layer are added together and fed into a second normalization layer. <br>
To prevent overfitting, dropout with <i>dropout_rate</i> is applied on the feed forward layer's output.

### 5. Linear Layer
A linear layer that transforms the dimension of the output from encoder block from <i>embed_dim</i> to <i>target_vocab_size</i>.

### 6. Softmax
Apply softmax function to get the probability of each token in the target vocabulary. <br>

### Summary
The transformer has:
- input: a sequence of integers. Dimension: <i>[seq_length]</i>.
- output: probability distribution over the target vocabulary for each token in the output sequence. Dimension: <i>[max_seq_length, target_vocab_size]</i>.

Note: in the actual implementation, we do not apply softmax function because when we use pytorch's CrossEntropyLoss() to calculate the loss and pytorch's CrossEntropyLoss() already applies softmax function before calculating the loss.

## Model Training
We train the model on train dataset and use cross entropy loss as objective function.

## Model Evaluation
We run the model on test dataset, and report the cross entropy loss.<br>
Besides, we also report prediction accuracy on token and sequence level.

## Model Tuning
We use optuna and a validation dataset to tune the model for the following parameters (numbers in brackets are possible values):
- embed_dim: [256, 512, 1024, 2048]
- n_heads: [1, 2, 4, 8]
- n_layers: [1, 2, 4, 8]
- d_ff = [512, 1024, 2048, 4096]
- dropout_rate: [0, 0.1, 0.2, 0.3, 0.4, 0.5]
- apply_mask : [True, False]

We observe that the most impactful parameters is the number of encoders (<i>n_layers</i>). When <i>n_layers = 1</i>, the accuracy on token level on the test dataset is about 66% after 20 training epochs. When <i>n_layers = 4</i>, the accuracy is over 99%.