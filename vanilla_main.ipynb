{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from VanillaTransformer import VanillaTransformer, CustomDataSet, model_tuning\n",
    "from config import SAMPLE_SIZE_BY_SEQ_LENGTH, MAX_SEQ_LENGTH, VOCAB_SIZE, RANDOM_STATE\n",
    "from model_utils import print_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_by_seq_length = SAMPLE_SIZE_BY_SEQ_LENGTH\n",
    "max_seq_length = MAX_SEQ_LENGTH     # also a parameter for the transformer model\n",
    "vocab_size = VOCAB_SIZE             # also a parameter for the transformer model\n",
    "random_state = RANDOM_STATE\n",
    "\n",
    "dataset = CustomDataSet(\n",
    "    sample_size_by_seq_length,\n",
    "    max_seq_length,\n",
    "    vocab_size,\n",
    "    random_state\n",
    ")\n",
    "\n",
    "# Split dataset into 20% training, 10% validation and 70% test\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.8, random_state=random_state)\n",
    "\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.875, random_state=random_state)\n",
    "\n",
    "batch_size = 8\n",
    "train_data_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "input_val = torch.stack([row[0] for row in val_data])\n",
    "target_val = torch.stack([row[1] for row in val_data])\n",
    "\n",
    "input_test = torch.stack([row[0] for row in test_data])\n",
    "target_test = torch.stack([row[1] for row in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset size\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([5, 6, 1, 0, 0]), tensor([10,  1,  6,  5,  0,  0])),\n",
       " (tensor([8, 8, 3, 4, 9]), tensor([10,  9,  4,  3,  8,  8])),\n",
       " (tensor([5, 9, 4, 1, 1]), tensor([10,  1,  1,  4,  9,  5])),\n",
       " (tensor([4, 2, 2, 8, 0]), tensor([10,  8,  2,  2,  4,  0])),\n",
       " (tensor([4, 5, 4, 5, 9]), tensor([10,  9,  5,  4,  5,  4])),\n",
       " (tensor([9, 7, 2, 8, 5]), tensor([10,  5,  8,  2,  7,  9]))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset example\n",
    "train_data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset size\n",
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([4, 4, 5, 1, 1]), tensor([10,  1,  1,  5,  4,  4])),\n",
       " (tensor([6, 8, 5, 8, 2]), tensor([10,  2,  8,  5,  8,  6])),\n",
       " (tensor([1, 2, 9, 1, 5]), tensor([10,  5,  1,  9,  2,  1])),\n",
       " (tensor([5, 3, 2, 9, 4]), tensor([10,  4,  9,  2,  3,  5])),\n",
       " (tensor([8, 9, 9, 3, 9]), tensor([10,  9,  3,  9,  9,  8])),\n",
       " (tensor([1, 1, 3, 4, 1]), tensor([10,  1,  4,  3,  1,  1]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset example\n",
    "val_data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3495"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset size\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([6, 6, 1, 5, 3]), tensor([10,  3,  5,  1,  6,  6])),\n",
       " (tensor([7, 4, 8, 5, 8]), tensor([10,  8,  5,  8,  4,  7])),\n",
       " (tensor([4, 6, 6, 5, 5]), tensor([10,  5,  5,  6,  6,  4])),\n",
       " (tensor([9, 2, 3, 1, 0]), tensor([10,  1,  3,  2,  9,  0])),\n",
       " (tensor([8, 8, 4, 3, 0]), tensor([10,  3,  4,  8,  8,  0])),\n",
       " (tensor([5, 1, 3, 8, 1]), tensor([10,  1,  8,  3,  1,  5]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset example\n",
    "test_data[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Vanilla Transformer.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = vocab_size + 1  # adding padding token 0\n",
    "tgt_vocab_size = vocab_size + 2  # adding padding token 0 and start of sequence token (which is 10 in this case)\n",
    "embed_dim = 512\n",
    "max_seq_length = max_seq_length\n",
    "n_heads = 8\n",
    "n_layers = 1\n",
    "d_ff = 2048\n",
    "dropout_rate = 0.1\n",
    "\n",
    "model = VanillaTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    max_seq_length=max_seq_length,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout_rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on train dataset and use cross entropy loss as objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.8096755743026733\n",
      "Epoch: 2, Loss: 1.4117408990859985\n",
      "Epoch: 3, Loss: 0.7652195692062378\n",
      "Epoch: 4, Loss: 0.8128213882446289\n",
      "Epoch: 5, Loss: 0.5296021699905396\n",
      "Epoch: 6, Loss: 0.521661639213562\n",
      "Epoch: 7, Loss: 0.2154308706521988\n",
      "Epoch: 8, Loss: 0.24926048517227173\n",
      "Epoch: 9, Loss: 0.2534395158290863\n",
      "Epoch: 10, Loss: 0.2331256866455078\n",
      "Epoch: 11, Loss: 0.27883580327033997\n",
      "Epoch: 12, Loss: 0.09254700690507889\n",
      "Epoch: 13, Loss: 0.27967706322669983\n",
      "Epoch: 14, Loss: 0.10516996681690216\n",
      "Epoch: 15, Loss: 0.10476777702569962\n",
      "Epoch: 16, Loss: 0.08430644124746323\n",
      "Epoch: 17, Loss: 0.06955530494451523\n",
      "Epoch: 18, Loss: 0.1066678911447525\n",
      "Epoch: 19, Loss: 0.0347219854593277\n",
      "Epoch: 20, Loss: 0.14663910865783691\n"
     ]
    }
   ],
   "source": [
    "train_loss = model.model_training(train_data_loader, epochs=20, lr=1e-5, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the model on test dataset, and report the cross entropy loss.<br>\n",
    "Besides, we also report prediction accuracy on token and sequence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.23523303866386414\n"
     ]
    }
   ],
   "source": [
    "out_test, loss = model.model_eval(input_test, target_test)\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target sequences\n",
    "target_test[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 4, 6],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted sequences\n",
    "pred_test = torch.argmax(out_test, dim=-1)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report accuracy on token level\n",
      "Number of wrong token predictions: 1073\n",
      "Number of total token predictions: 17475\n",
      "Token Accuracy: 93.8598%\n",
      "\n",
      "Report accuracy on sequence level\n",
      "Number of wrong sequence predictions: 580\n",
      "Number of total sequence predictions: 3495\n",
      "Sequence Accuracy: 83.4049%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(pred_test, target_test[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use optuna and a validation dataset to tune the model for the following parameters (numbers in brackets are possible values):\n",
    "- embed_dim: [256, 512, 1024, 2048]\n",
    "- n_heads: [1, 2, 4, 8]\n",
    "- n_layers: [1, 2, 4]\n",
    "- d_ff = [512, 1024, 2048, 4096]\n",
    "- dropout_rate: [0, 0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-03 12:19:43,910] A new study created in memory with name: no-name-98826c32-3c99-4942-9b2e-145f48a09734\n",
      "[I 2024-02-03 12:20:42,186] Trial 0 finished with value: 0.17483648657798767 and parameters: {'embed_dim': 256, 'n_heads': 4, 'n_layers': 4, 'd_ff': 1024, 'dropout_rate': 0.2}. Best is trial 0 with value: 0.17483648657798767.\n",
      "[I 2024-02-03 12:21:01,342] Trial 1 finished with value: 0.6769092679023743 and parameters: {'embed_dim': 512, 'n_heads': 8, 'n_layers': 1, 'd_ff': 512, 'dropout_rate': 0.5}. Best is trial 0 with value: 0.17483648657798767.\n",
      "[I 2024-02-03 12:21:31,251] Trial 2 finished with value: 0.04625502973794937 and parameters: {'embed_dim': 256, 'n_heads': 1, 'n_layers': 2, 'd_ff': 4096, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.04625502973794937.\n",
      "[I 2024-02-03 12:22:34,133] Trial 3 finished with value: 0.012402170337736607 and parameters: {'embed_dim': 512, 'n_heads': 2, 'n_layers': 4, 'd_ff': 1024, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:22:52,068] Trial 4 finished with value: 1.3539271354675293 and parameters: {'embed_dim': 256, 'n_heads': 4, 'n_layers': 1, 'd_ff': 512, 'dropout_rate': 0.5}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:24:00,887] Trial 5 finished with value: 0.10538636893033981 and parameters: {'embed_dim': 512, 'n_heads': 2, 'n_layers': 4, 'd_ff': 2048, 'dropout_rate': 0.5}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:25:46,636] Trial 6 finished with value: 0.1017519012093544 and parameters: {'embed_dim': 1024, 'n_heads': 4, 'n_layers': 4, 'd_ff': 4096, 'dropout_rate': 0.4}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:26:33,761] Trial 7 finished with value: 0.02315349504351616 and parameters: {'embed_dim': 1024, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:27:16,344] Trial 8 finished with value: 0.06930290907621384 and parameters: {'embed_dim': 1024, 'n_heads': 8, 'n_layers': 2, 'd_ff': 512, 'dropout_rate': 0.3}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:28:50,910] Trial 9 finished with value: 0.04319753125309944 and parameters: {'embed_dim': 1024, 'n_heads': 8, 'n_layers': 4, 'd_ff': 2048, 'dropout_rate': 0.4}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:31:53,922] Trial 10 finished with value: 0.051233865320682526 and parameters: {'embed_dim': 2048, 'n_heads': 2, 'n_layers': 4, 'd_ff': 1024, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:32:31,603] Trial 11 finished with value: 0.039367273449897766 and parameters: {'embed_dim': 512, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:34:21,625] Trial 12 finished with value: 0.01553761400282383 and parameters: {'embed_dim': 2048, 'n_heads': 2, 'n_layers': 2, 'd_ff': 1024, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:36:08,159] Trial 13 finished with value: 0.08182082325220108 and parameters: {'embed_dim': 2048, 'n_heads': 2, 'n_layers': 2, 'd_ff': 1024, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:37:01,135] Trial 14 finished with value: 0.5305908918380737 and parameters: {'embed_dim': 2048, 'n_heads': 1, 'n_layers': 1, 'd_ff': 1024, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:40:28,364] Trial 15 finished with value: 0.06940671801567078 and parameters: {'embed_dim': 2048, 'n_heads': 2, 'n_layers': 4, 'd_ff': 1024, 'dropout_rate': 0.2}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:41:03,750] Trial 16 finished with value: 0.061821579933166504 and parameters: {'embed_dim': 512, 'n_heads': 2, 'n_layers': 2, 'd_ff': 1024, 'dropout_rate': 0.1}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:42:13,887] Trial 17 finished with value: 0.04743993282318115 and parameters: {'embed_dim': 512, 'n_heads': 2, 'n_layers': 4, 'd_ff': 1024, 'dropout_rate': 0.3}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:43:58,050] Trial 18 finished with value: 0.1068953350186348 and parameters: {'embed_dim': 2048, 'n_heads': 1, 'n_layers': 2, 'd_ff': 1024, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n",
      "[I 2024-02-03 12:44:22,890] Trial 19 finished with value: 0.31291091442108154 and parameters: {'embed_dim': 512, 'n_heads': 2, 'n_layers': 1, 'd_ff': 4096, 'dropout_rate': 0}. Best is trial 3 with value: 0.012402170337736607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'embed_dim': 512, 'n_heads': 2, 'n_layers': 4, 'd_ff': 1024, 'dropout_rate': 0}\n"
     ]
    }
   ],
   "source": [
    "best_params = model_tuning(\n",
    "    train_data_loader,\n",
    "    input_val,\n",
    "    target_val,\n",
    "    vocab_size,\n",
    "    max_seq_length,\n",
    "    epochs=10,\n",
    "    n_trials=20,\n",
    ")\n",
    "print(\"Best params: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    embed_dim=best_params[\"embed_dim\"],\n",
    "    max_seq_length=max_seq_length,\n",
    "    n_heads=best_params[\"n_heads\"],\n",
    "    n_layers=best_params[\"n_layers\"],\n",
    "    d_ff=best_params[\"d_ff\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.3091050386428833\n",
      "Epoch: 2, Loss: 0.5252753496170044\n",
      "Epoch: 3, Loss: 0.17449423670768738\n",
      "Epoch: 4, Loss: 0.11421070247888565\n",
      "Epoch: 5, Loss: 0.06281331181526184\n",
      "Epoch: 6, Loss: 0.03583420440554619\n",
      "Epoch: 7, Loss: 0.014553552493453026\n",
      "Epoch: 8, Loss: 0.014586846344172955\n",
      "Epoch: 9, Loss: 0.008232591673731804\n",
      "Epoch: 10, Loss: 0.039465826004743576\n",
      "Epoch: 11, Loss: 0.005659711547195911\n",
      "Epoch: 12, Loss: 0.002602966036647558\n",
      "Epoch: 13, Loss: 0.0027130921371281147\n",
      "Epoch: 14, Loss: 0.002879694802686572\n",
      "Epoch: 15, Loss: 0.005742909386754036\n",
      "Epoch: 16, Loss: 0.0016401949105784297\n",
      "Epoch: 17, Loss: 0.0032453909516334534\n",
      "Epoch: 18, Loss: 0.0014023020630702376\n",
      "Epoch: 19, Loss: 0.0015753054758533835\n",
      "Epoch: 20, Loss: 0.0021481066942214966\n"
     ]
    }
   ],
   "source": [
    "train_loss = model.model_training(train_data_loader, epochs=20, lr=1e-5, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.005132450722157955\n"
     ]
    }
   ],
   "source": [
    "out_test, loss = model.model_eval(input_test, target_test)\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target sequences\n",
    "target_test[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted sequences\n",
    "pred_test = torch.argmax(out_test, dim=-1)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report accuracy on token level\n",
      "Number of wrong token predictions: 14\n",
      "Number of total token predictions: 17475\n",
      "Token Accuracy: 99.9199%\n",
      "\n",
      "Report accuracy on sequence level\n",
      "Number of wrong sequence predictions: 12\n",
      "Number of total sequence predictions: 3495\n",
      "Sequence Accuracy: 99.6567%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(pred_test, target_test[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "We referenced the implementation of the vanilla transformer from [datacamp](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
