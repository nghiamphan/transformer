{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from VanillaTransformer import VanillaTransformer, CustomDataSet, model_tuning\n",
    "from config import SAMPLE_SIZE_BY_SEQ_LENGTH, MAX_SEQ_LENGTH, VOCAB_SIZE, RANDOM_STATE\n",
    "from model_utils import print_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_by_seq_length = SAMPLE_SIZE_BY_SEQ_LENGTH\n",
    "max_seq_length = MAX_SEQ_LENGTH     # also a parameter for the transformer model\n",
    "vocab_size = VOCAB_SIZE             # also a parameter for the transformer model\n",
    "random_state = RANDOM_STATE\n",
    "\n",
    "dataset = CustomDataSet(\n",
    "    sample_size_by_seq_length,\n",
    "    max_seq_length,\n",
    "    vocab_size,\n",
    "    random_state\n",
    ")\n",
    "\n",
    "# Split dataset into 20% training, 10% validation and 70% test\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.8, random_state=random_state)\n",
    "\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.875, random_state=random_state)\n",
    "\n",
    "batch_size = 8\n",
    "train_data_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "input_val = torch.stack([row[0] for row in val_data])\n",
    "target_val = torch.stack([row[1] for row in val_data])\n",
    "\n",
    "input_test = torch.stack([row[0] for row in test_data])\n",
    "target_test = torch.stack([row[1] for row in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset size\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([5, 6, 1, 0, 0]), tensor([10,  1,  6,  5,  0,  0])),\n",
       " (tensor([8, 8, 3, 4, 9]), tensor([10,  9,  4,  3,  8,  8])),\n",
       " (tensor([5, 9, 4, 1, 1]), tensor([10,  1,  1,  4,  9,  5])),\n",
       " (tensor([4, 2, 2, 8, 0]), tensor([10,  8,  2,  2,  4,  0])),\n",
       " (tensor([4, 5, 4, 5, 9]), tensor([10,  9,  5,  4,  5,  4])),\n",
       " (tensor([9, 7, 2, 8, 5]), tensor([10,  5,  8,  2,  7,  9]))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset example\n",
    "train_data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset size\n",
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([4, 4, 5, 1, 1]), tensor([10,  1,  1,  5,  4,  4])),\n",
       " (tensor([6, 8, 5, 8, 2]), tensor([10,  2,  8,  5,  8,  6])),\n",
       " (tensor([1, 2, 9, 1, 5]), tensor([10,  5,  1,  9,  2,  1])),\n",
       " (tensor([5, 3, 2, 9, 4]), tensor([10,  4,  9,  2,  3,  5])),\n",
       " (tensor([8, 9, 9, 3, 9]), tensor([10,  9,  3,  9,  9,  8])),\n",
       " (tensor([1, 1, 3, 4, 1]), tensor([10,  1,  4,  3,  1,  1]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset example\n",
    "val_data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3495"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset size\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([6, 6, 1, 5, 3]), tensor([10,  3,  5,  1,  6,  6])),\n",
       " (tensor([7, 4, 8, 5, 8]), tensor([10,  8,  5,  8,  4,  7])),\n",
       " (tensor([4, 6, 6, 5, 5]), tensor([10,  5,  5,  6,  6,  4])),\n",
       " (tensor([9, 2, 3, 1, 0]), tensor([10,  1,  3,  2,  9,  0])),\n",
       " (tensor([8, 8, 4, 3, 0]), tensor([10,  3,  4,  8,  8,  0])),\n",
       " (tensor([5, 1, 3, 8, 1]), tensor([10,  1,  8,  3,  1,  5]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset example\n",
    "test_data[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Vanilla Transformer.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = vocab_size + 1  # adding padding token 0\n",
    "tgt_vocab_size = vocab_size + 2  # adding padding token 0 and start of sequence token (which is 10 in this case)\n",
    "embed_dim = 512\n",
    "max_seq_length = max_seq_length\n",
    "n_heads = 8\n",
    "n_layers = 1\n",
    "d_ff = 2048\n",
    "dropout_rate = 0.1\n",
    "\n",
    "model = VanillaTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    max_seq_length=max_seq_length,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout_rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on train dataset and use cross entropy loss as objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.8272465467453003\n",
      "Epoch: 2, Loss: 1.4297220706939697\n",
      "Epoch: 3, Loss: 0.7493414282798767\n",
      "Epoch: 4, Loss: 0.8159533739089966\n",
      "Epoch: 5, Loss: 0.5254016518592834\n",
      "Epoch: 6, Loss: 0.4927596151828766\n",
      "Epoch: 7, Loss: 0.1982528418302536\n",
      "Epoch: 8, Loss: 0.11651254445314407\n",
      "Epoch: 9, Loss: 0.14379571378231049\n",
      "Epoch: 10, Loss: 0.09038591384887695\n",
      "Epoch: 11, Loss: 0.07312215119600296\n",
      "Epoch: 12, Loss: 0.027223652228713036\n",
      "Epoch: 13, Loss: 0.10246894508600235\n",
      "Epoch: 14, Loss: 0.08674386888742447\n",
      "Epoch: 15, Loss: 0.0378841757774353\n",
      "Epoch: 16, Loss: 0.028905488550662994\n",
      "Epoch: 17, Loss: 0.02624937891960144\n",
      "Epoch: 18, Loss: 0.01564835198223591\n",
      "Epoch: 19, Loss: 0.01891220360994339\n",
      "Epoch: 20, Loss: 0.03639163821935654\n"
     ]
    }
   ],
   "source": [
    "train_loss = model.model_training(train_data_loader, epochs=20, lr=1e-5, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the model on test dataset, and report the cross entropy loss.<br>\n",
    "Besides, we also report prediction accuracy on token and sequence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07680976390838623\n"
     ]
    }
   ],
   "source": [
    "out_test, loss = model.model_eval(input_test, target_test)\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target sequences\n",
    "target_test[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted sequences\n",
    "pred_test = torch.argmax(out_test, dim=-1)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report accuracy on token level\n",
      "Number of wrong token predictions: 278\n",
      "Number of total token predictions: 17475\n",
      "Token Accuracy: 98.4092%\n",
      "\n",
      "Report accuracy on sequence level\n",
      "Number of wrong sequence predictions: 157\n",
      "Number of total sequence predictions: 3495\n",
      "Sequence Accuracy: 95.5079%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(pred_test, target_test[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use optuna and a validation dataset to tune the model for the following parameters (numbers in brackets are possible values):\n",
    "- embed_dim: [256, 512, 1024, 2048]\n",
    "- n_heads: [1, 2, 4, 8]\n",
    "- n_layers: [1, 2, 4]\n",
    "- d_ff = [512, 1024, 2048, 4096]\n",
    "- dropout_rate: [0, 0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-03 11:30:17,570] A new study created in memory with name: no-name-02f5a1ee-5a5f-4627-b1c5-c4fe85e7e428\n",
      "[I 2024-02-03 11:32:36,898] Trial 0 finished with value: 0.13908790051937103 and parameters: {'embed_dim': 2048, 'n_heads': 4, 'n_layers': 4, 'd_ff': 1024, 'dropout_rate': 0.2}. Best is trial 0 with value: 0.13908790051937103.\n",
      "[I 2024-02-03 11:33:01,953] Trial 1 finished with value: 0.5560218095779419 and parameters: {'embed_dim': 256, 'n_heads': 8, 'n_layers': 2, 'd_ff': 1024, 'dropout_rate': 0.1}. Best is trial 0 with value: 0.13908790051937103.\n",
      "[I 2024-02-03 11:33:54,679] Trial 2 finished with value: 0.2687663733959198 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 4, 'd_ff': 2048, 'dropout_rate': 0.4}. Best is trial 0 with value: 0.13908790051937103.\n",
      "[I 2024-02-03 11:34:48,479] Trial 3 finished with value: 0.6852685213088989 and parameters: {'embed_dim': 256, 'n_heads': 8, 'n_layers': 4, 'd_ff': 4096, 'dropout_rate': 0.5}. Best is trial 0 with value: 0.13908790051937103.\n",
      "[I 2024-02-03 11:35:10,573] Trial 4 finished with value: 0.0659429058432579 and parameters: {'embed_dim': 1024, 'n_heads': 4, 'n_layers': 1, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:35:30,762] Trial 5 finished with value: 0.07125402241945267 and parameters: {'embed_dim': 1024, 'n_heads': 8, 'n_layers': 1, 'd_ff': 512, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:35:51,068] Trial 6 finished with value: 0.16823235154151917 and parameters: {'embed_dim': 1024, 'n_heads': 2, 'n_layers': 1, 'd_ff': 512, 'dropout_rate': 0.3}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:37:29,180] Trial 7 finished with value: 0.07520872354507446 and parameters: {'embed_dim': 1024, 'n_heads': 2, 'n_layers': 4, 'd_ff': 4096, 'dropout_rate': 0.2}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:37:57,381] Trial 8 finished with value: 0.09093645215034485 and parameters: {'embed_dim': 512, 'n_heads': 4, 'n_layers': 2, 'd_ff': 512, 'dropout_rate': 0.2}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:39:04,204] Trial 9 finished with value: 0.11428748816251755 and parameters: {'embed_dim': 512, 'n_heads': 8, 'n_layers': 4, 'd_ff': 4096, 'dropout_rate': 0.4}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:39:45,683] Trial 10 finished with value: 0.24052076041698456 and parameters: {'embed_dim': 2048, 'n_heads': 1, 'n_layers': 1, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:40:06,707] Trial 11 finished with value: 0.0833483412861824 and parameters: {'embed_dim': 1024, 'n_heads': 4, 'n_layers': 1, 'd_ff': 512, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:40:27,966] Trial 12 finished with value: 0.07877179235219955 and parameters: {'embed_dim': 1024, 'n_heads': 1, 'n_layers': 1, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:40:50,355] Trial 13 finished with value: 0.09199389070272446 and parameters: {'embed_dim': 1024, 'n_heads': 8, 'n_layers': 1, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:41:11,266] Trial 14 finished with value: 0.08033695816993713 and parameters: {'embed_dim': 1024, 'n_heads': 4, 'n_layers': 1, 'd_ff': 512, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:41:34,273] Trial 15 finished with value: 0.10436727851629257 and parameters: {'embed_dim': 1024, 'n_heads': 8, 'n_layers': 1, 'd_ff': 2048, 'dropout_rate': 0.3}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:41:55,232] Trial 16 finished with value: 0.08313161879777908 and parameters: {'embed_dim': 1024, 'n_heads': 4, 'n_layers': 1, 'd_ff': 512, 'dropout_rate': 0.1}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:43:08,443] Trial 17 finished with value: 0.10979248583316803 and parameters: {'embed_dim': 2048, 'n_heads': 1, 'n_layers': 2, 'd_ff': 1024, 'dropout_rate': 0.5}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:43:24,019] Trial 18 finished with value: 0.20611239969730377 and parameters: {'embed_dim': 512, 'n_heads': 8, 'n_layers': 1, 'd_ff': 512, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n",
      "[I 2024-02-03 11:43:46,208] Trial 19 finished with value: 0.1022014170885086 and parameters: {'embed_dim': 1024, 'n_heads': 4, 'n_layers': 1, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 4 with value: 0.0659429058432579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'embed_dim': 1024, 'n_heads': 4, 'n_layers': 1, 'd_ff': 2048, 'dropout_rate': 0}\n"
     ]
    }
   ],
   "source": [
    "best_params = model_tuning(\n",
    "    train_data_loader,\n",
    "    input_val,\n",
    "    target_val,\n",
    "    vocab_size,\n",
    "    max_seq_length,\n",
    "    epochs=10,\n",
    "    n_trials=20,\n",
    ")\n",
    "print(\"Best params: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    embed_dim=best_params[\"embed_dim\"],\n",
    "    max_seq_length=max_seq_length,\n",
    "    n_heads=best_params[\"n_heads\"],\n",
    "    n_layers=best_params[\"n_layers\"],\n",
    "    d_ff=best_params[\"d_ff\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.2330305576324463\n",
      "Epoch: 2, Loss: 0.776840090751648\n",
      "Epoch: 3, Loss: 0.22060611844062805\n",
      "Epoch: 4, Loss: 0.12049145996570587\n",
      "Epoch: 5, Loss: 0.05393253266811371\n",
      "Epoch: 6, Loss: 0.06661316752433777\n",
      "Epoch: 7, Loss: 0.0707632303237915\n",
      "Epoch: 8, Loss: 0.10828747600317001\n",
      "Epoch: 9, Loss: 0.1326436847448349\n",
      "Epoch: 10, Loss: 0.02553623355925083\n",
      "Epoch: 11, Loss: 0.012216189876198769\n",
      "Epoch: 12, Loss: 0.023078931495547295\n",
      "Epoch: 13, Loss: 0.01660727895796299\n",
      "Epoch: 14, Loss: 0.016780806705355644\n",
      "Epoch: 15, Loss: 0.006747760809957981\n",
      "Epoch: 16, Loss: 0.007753516081720591\n",
      "Epoch: 17, Loss: 0.010200251825153828\n",
      "Epoch: 18, Loss: 0.002656022785231471\n",
      "Epoch: 19, Loss: 0.0025182601530104876\n",
      "Epoch: 20, Loss: 0.014368832111358643\n"
     ]
    }
   ],
   "source": [
    "train_loss = model.model_training(train_data_loader, epochs=20, lr=1e-5, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0772976353764534\n"
     ]
    }
   ],
   "source": [
    "out_test, loss = model.model_eval(input_test, target_test)\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target sequences\n",
    "target_test[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 9, 3, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted sequences\n",
    "pred_test = torch.argmax(out_test, dim=-1)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report accuracy on token level\n",
      "Number of wrong token predictions: 280\n",
      "Number of total token predictions: 17475\n",
      "Token Accuracy: 98.3977%\n",
      "\n",
      "Report accuracy on sequence level\n",
      "Number of wrong sequence predictions: 142\n",
      "Number of total sequence predictions: 3495\n",
      "Sequence Accuracy: 95.9371%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(pred_test, target_test[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "We referenced the implementation of the vanilla transformer from [datacamp](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
