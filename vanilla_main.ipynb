{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from VanillaTransformer import VanillaTransformer, CustomDataSet, model_tuning\n",
    "from config import SAMPLE_SIZE_BY_SEQ_LENGTH, MAX_SEQ_LENGTH, VOCAB_SIZE, RANDOM_STATE\n",
    "from model_utils import print_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size_by_seq_length = SAMPLE_SIZE_BY_SEQ_LENGTH\n",
    "max_seq_length = MAX_SEQ_LENGTH     # also a parameter for the transformer model\n",
    "vocab_size = VOCAB_SIZE             # also a parameter for the transformer model\n",
    "random_state = RANDOM_STATE\n",
    "\n",
    "dataset = CustomDataSet(\n",
    "    sample_size_by_seq_length,\n",
    "    max_seq_length,\n",
    "    vocab_size,\n",
    "    random_state\n",
    ")\n",
    "\n",
    "# Split dataset into 20% training, 10% validation and 70% test\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.8, random_state=random_state)\n",
    "\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.875, random_state=random_state)\n",
    "\n",
    "batch_size = 1\n",
    "train_data_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "input_val = torch.stack([row[0] for row in val_data])\n",
    "target_val = torch.stack([row[1] for row in val_data])\n",
    "\n",
    "input_test = torch.stack([row[0] for row in test_data])\n",
    "target_test = torch.stack([row[1] for row in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset size\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([5, 6, 1, 0, 0]), tensor([10,  1,  6,  5,  0,  0])),\n",
       " (tensor([8, 8, 3, 4, 9]), tensor([10,  9,  4,  3,  8,  8])),\n",
       " (tensor([5, 9, 4, 1, 1]), tensor([10,  1,  1,  4,  9,  5])),\n",
       " (tensor([4, 2, 2, 8, 0]), tensor([10,  8,  2,  2,  4,  0])),\n",
       " (tensor([4, 5, 4, 5, 9]), tensor([10,  9,  5,  4,  5,  4])),\n",
       " (tensor([9, 7, 2, 8, 5]), tensor([10,  5,  8,  2,  7,  9]))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset example\n",
    "train_data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset size\n",
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([4, 4, 5, 1, 1]), tensor([10,  1,  1,  5,  4,  4])),\n",
       " (tensor([6, 8, 5, 8, 2]), tensor([10,  2,  8,  5,  8,  6])),\n",
       " (tensor([1, 2, 9, 1, 5]), tensor([10,  5,  1,  9,  2,  1])),\n",
       " (tensor([5, 3, 2, 9, 4]), tensor([10,  4,  9,  2,  3,  5])),\n",
       " (tensor([8, 9, 9, 3, 9]), tensor([10,  9,  3,  9,  9,  8])),\n",
       " (tensor([1, 1, 3, 4, 1]), tensor([10,  1,  4,  3,  1,  1]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset example\n",
    "val_data[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3495"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset size\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([6, 6, 1, 5, 3]), tensor([10,  3,  5,  1,  6,  6])),\n",
       " (tensor([7, 4, 8, 5, 8]), tensor([10,  8,  5,  8,  4,  7])),\n",
       " (tensor([4, 6, 6, 5, 5]), tensor([10,  5,  5,  6,  6,  4])),\n",
       " (tensor([9, 2, 3, 1, 0]), tensor([10,  1,  3,  2,  9,  0])),\n",
       " (tensor([8, 8, 4, 3, 0]), tensor([10,  3,  4,  8,  8,  0])),\n",
       " (tensor([5, 1, 3, 8, 1]), tensor([10,  1,  8,  3,  1,  5]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset example\n",
    "test_data[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Vanilla Transformer.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = vocab_size + 1  # adding padding token 0\n",
    "tgt_vocab_size = vocab_size + 2  # adding padding token 0 and start of sequence token (which is 10 in this case)\n",
    "embed_dim = 512\n",
    "max_seq_length = max_seq_length\n",
    "n_heads = 8\n",
    "n_layers = 1\n",
    "d_ff = 2048\n",
    "dropout_rate = 0.1\n",
    "\n",
    "model = VanillaTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    max_seq_length=max_seq_length,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout_rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on train dataset and use cross entropy loss as objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.142053484916687\n",
      "Epoch: 2, Loss: 0.3380657434463501\n",
      "Epoch: 3, Loss: 0.0802735686302185\n",
      "Epoch: 4, Loss: 0.04625968262553215\n",
      "Epoch: 5, Loss: 0.05859767645597458\n",
      "Epoch: 6, Loss: 0.5355597138404846\n",
      "Epoch: 7, Loss: 0.01627356745302677\n",
      "Epoch: 8, Loss: 0.0002961803402286023\n",
      "Epoch: 9, Loss: 0.09511144459247589\n",
      "Epoch: 10, Loss: 0.020626284182071686\n",
      "Epoch: 11, Loss: 0.04849330708384514\n",
      "Epoch: 12, Loss: 0.04877958446741104\n",
      "Epoch: 13, Loss: 0.012528476305305958\n",
      "Epoch: 14, Loss: 0.008336951024830341\n",
      "Epoch: 15, Loss: 0.004708576016128063\n",
      "Epoch: 16, Loss: 0.01055431179702282\n",
      "Epoch: 17, Loss: 0.02264772541821003\n",
      "Epoch: 18, Loss: 0.0021700263023376465\n",
      "Epoch: 19, Loss: 0.00041222345316782594\n",
      "Epoch: 20, Loss: 0.0025126540567725897\n"
     ]
    }
   ],
   "source": [
    "train_loss = model.model_training(train_data_loader, epochs=20, lr=1e-5, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the model on test dataset, and report the cross entropy loss.<br>\n",
    "Besides, we also report prediction accuracy on token and sequence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.30782240629196167\n"
     ]
    }
   ],
   "source": [
    "out_test, loss = model.model_eval(input_test, target_test, batch_test=False)\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target sequences\n",
    "target_test[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 8, 3, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted sequences\n",
    "pred_test = torch.argmax(out_test, dim=-1)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report accuracy on token level\n",
      "Number of wrong token predictions: 736\n",
      "Number of total token predictions: 17475\n",
      "Token Accuracy: 95.7883%\n",
      "\n",
      "Report accuracy on sequence level\n",
      "Number of wrong sequence predictions: 381\n",
      "Number of total sequence predictions: 3495\n",
      "Sequence Accuracy: 89.0987%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(pred_test, target_test[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use optuna and a validation dataset to tune the model for the following parameters (numbers in brackets are possible values):\n",
    "- embed_dim: [256, 512, 1024, 2048]\n",
    "- n_heads: [1, 2, 4, 8]\n",
    "- n_layers: [1, 2, 4]\n",
    "- d_ff = [512, 1024, 2048, 4096]\n",
    "- dropout_rate: [0, 0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-03 22:19:37,836] A new study created in memory with name: no-name-241ede8a-822b-445e-945c-41d53c70a7d4\n",
      "[I 2024-02-03 22:25:39,625] Trial 0 finished with value: 0.21189600229263306 and parameters: {'embed_dim': 256, 'n_heads': 1, 'n_layers': 4, 'd_ff': 1024, 'dropout_rate': 0.5}. Best is trial 0 with value: 0.21189600229263306.\n",
      "[I 2024-02-03 22:32:16,403] Trial 1 finished with value: 0.6697384715080261 and parameters: {'embed_dim': 2048, 'n_heads': 1, 'n_layers': 1, 'd_ff': 4096, 'dropout_rate': 0.4}. Best is trial 0 with value: 0.21189600229263306.\n",
      "[I 2024-02-03 22:43:22,362] Trial 2 finished with value: 0.11623693257570267 and parameters: {'embed_dim': 2048, 'n_heads': 8, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 22:45:53,534] Trial 3 finished with value: 0.4158693850040436 and parameters: {'embed_dim': 512, 'n_heads': 8, 'n_layers': 1, 'd_ff': 1024, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 22:54:31,146] Trial 4 finished with value: 0.2131304144859314 and parameters: {'embed_dim': 256, 'n_heads': 8, 'n_layers': 4, 'd_ff': 4096, 'dropout_rate': 0.4}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 22:59:31,420] Trial 5 finished with value: 0.5247231721878052 and parameters: {'embed_dim': 2048, 'n_heads': 8, 'n_layers': 1, 'd_ff': 1024, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 23:04:22,077] Trial 6 finished with value: 0.436079204082489 and parameters: {'embed_dim': 1024, 'n_heads': 4, 'n_layers': 2, 'd_ff': 512, 'dropout_rate': 0.4}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 23:06:19,049] Trial 7 finished with value: 0.39522311091423035 and parameters: {'embed_dim': 512, 'n_heads': 4, 'n_layers': 1, 'd_ff': 1024, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 23:09:43,019] Trial 8 finished with value: 0.13033746182918549 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 2, 'd_ff': 1024, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 23:13:16,692] Trial 9 finished with value: 0.29712235927581787 and parameters: {'embed_dim': 512, 'n_heads': 4, 'n_layers': 2, 'd_ff': 1024, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 23:23:42,238] Trial 10 finished with value: 0.25799429416656494 and parameters: {'embed_dim': 2048, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.11623693257570267.\n",
      "[I 2024-02-03 23:27:10,963] Trial 11 finished with value: 0.10121183842420578 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 11 with value: 0.10121183842420578.\n",
      "[I 2024-02-03 23:32:37,089] Trial 12 finished with value: 0.23334041237831116 and parameters: {'embed_dim': 1024, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 11 with value: 0.10121183842420578.\n",
      "[I 2024-02-03 23:35:56,350] Trial 13 finished with value: 0.10931775718927383 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0.3}. Best is trial 11 with value: 0.10121183842420578.\n",
      "[I 2024-02-03 23:39:15,015] Trial 14 finished with value: 0.17843331396579742 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0.3}. Best is trial 11 with value: 0.10121183842420578.\n",
      "[I 2024-02-03 23:42:31,593] Trial 15 finished with value: 0.09537368267774582 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0}. Best is trial 15 with value: 0.09537368267774582.\n",
      "[I 2024-02-03 23:48:25,823] Trial 16 finished with value: 0.09628859907388687 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 4, 'd_ff': 512, 'dropout_rate': 0}. Best is trial 15 with value: 0.09537368267774582.\n",
      "[I 2024-02-03 23:54:23,618] Trial 17 finished with value: 0.10215543955564499 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 4, 'd_ff': 512, 'dropout_rate': 0}. Best is trial 15 with value: 0.09537368267774582.\n",
      "[I 2024-02-04 00:00:20,541] Trial 18 finished with value: 0.10124044120311737 and parameters: {'embed_dim': 256, 'n_heads': 2, 'n_layers': 4, 'd_ff': 512, 'dropout_rate': 0}. Best is trial 15 with value: 0.09537368267774582.\n",
      "[I 2024-02-04 00:09:08,461] Trial 19 finished with value: 0.46959808468818665 and parameters: {'embed_dim': 1024, 'n_heads': 1, 'n_layers': 4, 'd_ff': 512, 'dropout_rate': 0}. Best is trial 15 with value: 0.09537368267774582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'embed_dim': 256, 'n_heads': 2, 'n_layers': 2, 'd_ff': 2048, 'dropout_rate': 0}\n"
     ]
    }
   ],
   "source": [
    "best_params = model_tuning(\n",
    "    train_data_loader,\n",
    "    input_val,\n",
    "    target_val,\n",
    "    vocab_size,\n",
    "    max_seq_length,\n",
    "    epochs=10,\n",
    "    n_trials=20,\n",
    ")\n",
    "print(\"Best params: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Setup with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    embed_dim=best_params[\"embed_dim\"],\n",
    "    max_seq_length=max_seq_length,\n",
    "    n_heads=best_params[\"n_heads\"],\n",
    "    n_layers=best_params[\"n_layers\"],\n",
    "    d_ff=best_params[\"d_ff\"],\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.9099915623664856\n",
      "Epoch: 2, Loss: 0.07689107954502106\n",
      "Epoch: 3, Loss: 0.056292109191417694\n",
      "Epoch: 4, Loss: 0.0389518104493618\n",
      "Epoch: 5, Loss: 0.011284986510872841\n",
      "Epoch: 6, Loss: 0.009724492207169533\n",
      "Epoch: 7, Loss: 0.0022302691359072924\n",
      "Epoch: 8, Loss: 0.01828012987971306\n",
      "Epoch: 9, Loss: 0.004179255571216345\n",
      "Epoch: 10, Loss: 0.0011964959558099508\n",
      "Epoch: 11, Loss: 0.015082542784512043\n",
      "Epoch: 12, Loss: 0.0010234324727207422\n",
      "Epoch: 13, Loss: 0.000935417483560741\n",
      "Epoch: 14, Loss: 0.0008851202437654138\n",
      "Epoch: 15, Loss: 0.0008494289359077811\n",
      "Epoch: 16, Loss: 0.0006446017068810761\n",
      "Epoch: 17, Loss: 0.0005978471017442644\n",
      "Epoch: 18, Loss: 0.00036677654134109616\n",
      "Epoch: 19, Loss: 0.0004251152859069407\n",
      "Epoch: 20, Loss: 0.00027301558293402195\n"
     ]
    }
   ],
   "source": [
    "train_loss = model.model_training(train_data_loader, epochs=20, lr=1e-5, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08388354629278183\n"
     ]
    }
   ],
   "source": [
    "out_test, loss = model.model_eval(input_test, target_test, batch_test=False)\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target sequences\n",
    "target_test[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 1, 6, 6],\n",
       "        [8, 5, 8, 4, 7],\n",
       "        [5, 5, 6, 6, 4],\n",
       "        ...,\n",
       "        [5, 8, 3, 9, 0],\n",
       "        [2, 1, 8, 9, 4],\n",
       "        [1, 4, 6, 5, 4]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted sequences\n",
    "pred_test = torch.argmax(out_test, dim=-1)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report accuracy on token level\n",
      "Number of wrong token predictions: 0\n",
      "Number of total token predictions: 17475\n",
      "Token Accuracy: 100.0000%\n",
      "\n",
      "Report accuracy on sequence level\n",
      "Number of wrong sequence predictions: 0\n",
      "Number of total sequence predictions: 3495\n",
      "Sequence Accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(pred_test, target_test[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "We referenced the implementation of the vanilla transformer from [datacamp](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
