import torch
import torch.nn.functional as F

from torch.utils.data import Dataset
from config import SAMPLE_SIZE_BY_SEQ_LENGTH, MAX_SEQ_LENGTH, VOCAB_SIZE, RANDOM_STATE


class CustomDataSet(Dataset):
    """
    Arguments
    ---------
    sample_size_by_seq_length: dict[seq_length: sample_size]
        This is a dictionary where a key is a sequence length and its value is the number
        of sequences to be generated by that sequence length.
        Example: {1: 4, 2: 20, 3: 100, 4: 400, 5: 1000}
    max_seq_length: int
        the maximum length of each sequence
    vocab_size: int
        the size of vocabulary to be generated
    random_state: int
    """

    def __init__(
        self,
        sample_size_by_seq_length: dict = SAMPLE_SIZE_BY_SEQ_LENGTH,
        max_seq_length: int = MAX_SEQ_LENGTH,
        vocab_size: int = VOCAB_SIZE,
        random_state: int = RANDOM_STATE,
    ):
        torch.manual_seed(random_state)

        self.data = torch.empty(0, 2, max_seq_length, dtype=torch.int)

        for seq_length, sample_size in sample_size_by_seq_length.items():
            # Dimension of input and target: [sample_size, max_seq_length]
            input, target = generate_sample(
                sample_size,
                max_seq_length,
                seq_length,
                vocab_size,
            )
            # [sample_size, 2, max_seq_length]
            combined_input_target = torch.cat((input.unsqueeze(1), target.unsqueeze(1)), dim=1)

            # [total_sample_size, 2, max_seq_length]
            self.data = torch.cat((self.data, combined_input_target), dim=0)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        input, target = self.data[index]
        return input, target


def generate_sample(sample_size: int, max_seq_length: int, seq_length: int, vocab_size: int):
    """
    Arguments
    ---------
    sample_size: int
        number of sequences to generate
    max_seq_length: int
        the maximum length of each sequence
    seq_length: int
        the length of each sequence. If seq_length < max_seq_length, each sequence will be padded with 0 at the end.
    vocab_size: int
        the size of vocabulary to be generated (not including padding token 0)

    Returns
    -------
    input: torch.Tensor [sample_size, max_seq_length]
    target: torch.Tensor [sample_size, max_seq_length]
    """
    # generate input sequences consisting of numbers from 1 to vocab_size
    input = torch.randint(low=1, high=vocab_size + 1, size=(sample_size, seq_length))

    # keep only uniquely generated inputs
    input = torch.unique(input, dim=0)

    # generate target sequences which are reverses of input sequences
    target = torch.flip(input, dims=(1,))

    # pad the input and target at the end with 0s so that each sequence has length = max_seq_length
    input = F.pad(input, (0, max_seq_length - seq_length, 0, 0), value=0)
    target = F.pad(target, (0, max_seq_length - seq_length, 0, 0), value=0)

    return input, target
